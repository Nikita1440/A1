{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059e805a-5928-48a0-8988-42e37dd319c6",
   "metadata": {},
   "source": [
    "1. Using a graph to illustrate slope and intercept, define basic linear regression.\n",
    "2. In a graph, explain the terms rise, run, and slope.\n",
    "3. Use a graph to demonstrate slope, linear positive slope, and linear negative slope, as well as the\n",
    "different conditions that contribute to the slope.\n",
    "\n",
    "4. Use a graph to demonstrate curve linear negative slope and curve linear positive slope.\n",
    "\n",
    "5. Use a graph to show the maximum and low points of curves.\n",
    "\n",
    "6. Use the formulas for a and b to explain ordinary least squares.\n",
    "\n",
    "7. Provide a step-by-step explanation of the OLS algorithm.\n",
    "\n",
    "8. What is the regression&#39;s standard error? To represent the same, make a graph.\n",
    "\n",
    "9. Provide an example of multiple linear regression.\n",
    "\n",
    "10. Describe the regression analysis assumptions and the BLUE principle.\n",
    "\n",
    "11. Describe two major issues with regression analysis.\n",
    "\n",
    "12. How can the linear regression model&#39;s accuracy be improved?\n",
    "\n",
    "13. Using an example, describe the polynomial regression model in detail.\n",
    "\n",
    "14. Provide a detailed explanation of logistic regression.\n",
    "\n",
    "15. What are the logistic regression assumptions?\n",
    "\n",
    "16. Go through the details of maximum likelihood estimation.\n",
    "\n",
    "# 1\n",
    "Basic linear regression is a statistical technique used to model the relationship between two variables by fitting a straight line to the data. It aims to find the best-fitting line that minimizes the difference between the observed data points and the predicted values on the line. In terms of a graph, linear regression involves finding the slope and intercept of the line that represents the relationship between the variables.\n",
    "Here's an example of a graph illustrating basic linear regression\n",
    ":\n",
    "\n",
    "       Y\n",
    "       ^\n",
    "       |\n",
    "       |       .    .\n",
    "       |     .        .\n",
    "       |   .            .\n",
    "       | .                .\n",
    "       --------------------> X\n",
    "\n",
    "In the graph above, the points represent the observed data. The line represents the best-fitting line obtained through linear regression. The slope of the line determines the relationship between the X and Y variables, while the intercept represents the value of Y when X is zero.\n",
    "\n",
    "# 2\n",
    "In a graph, the terms \"rise,\" \"run,\" and \"slope\" are related to the steepness or inclination of a line. Here's how they are represented on a graph:\n",
    "arduino\n",
    "      Y\n",
    "      ^\n",
    "      |         . P(x2, y2)\n",
    "      |       .\n",
    "      |     .\n",
    "      |   .\n",
    "      | .                slope = rise / run\n",
    "      |_______________>\n",
    "             X        run\n",
    "\n",
    "Rise: It refers to the vertical change between two points on a line. In the graph above, the rise is the vertical distance between the two points (P) on the line.\n",
    "\n",
    "Run: It represents the horizontal change between two points on a line. In the graph, the run is the horizontal distance between the two points (P) on the line.\n",
    "\n",
    "Slope: It is the ratio of the rise (vertical change) to the run (horizontal change) between two points on a line. It indicates the steepness or inclination of the line. The slope is calculated by dividing the rise by the run.\n",
    "\n",
    "# 3\n",
    "Here's a graph demonstrating slope, linear positive slope, linear negative slope, and the different conditions that contribute to the slope:\n",
    "markdown\n",
    "      Y\n",
    "      ^\n",
    "      |       . P2(x2, y2)\n",
    "      |     .\n",
    "      |   .\n",
    "      | .                Positive Slope\n",
    "      |____________________\n",
    "                           \\       /\n",
    "                            \\     /\n",
    "                             \\   /\n",
    "                              \\ /\n",
    "                               X\n",
    "\n",
    "      Y\n",
    "      ^\n",
    "      | .                   Negative Slope\n",
    "      |   \\               /\n",
    "      |     \\           /\n",
    "      |       \\       /\n",
    "      |_________\\\n",
    "                  X\n",
    "\n",
    "Positive Slope: In the graph above, a line with a positive slope rises from left to right. It indicates a positive relationship between the X and Y variables. As the X values increase, the Y values also increase.\n",
    "\n",
    "Negative Slope: The line with a negative slope falls from left to right. It represents a negative relationship between the X and Y variables. As the X values increase, the Y values decrease.\n",
    "\n",
    "The slope of a line depends on the direction and steepness of the line. A positive slope has a positive value, while a negative slope has a negative value. The magnitude of the slope indicates the steepness of the line.\n",
    "# 4\n",
    "      Y\n",
    "      ^\n",
    "      |   \\              /\n",
    "      |     \\          /\n",
    "      |       \\      /\n",
    "      |         \\  /                  Curve Linear Positive Slope\n",
    "      |___________\\\n",
    "                    X\n",
    "\n",
    "      Y\n",
    "      ^\n",
    "      |         /  \\\n",
    "      |       /      \\\n",
    "      |     /          \\\n",
    "      |\n",
    "\n",
    "Curve Linear Positive Slope: In the graph above, the line has a curved shape with a positive slope. It indicates a positive relationship between the X and Y variables, where the Y values generally increase as the X values increase. However, the relationship is not strictly linear and exhibits a curvature.\n",
    "\n",
    "Curve Linear Negative Slope: The line in the graph has a curved shape with a negative slope. It represents a negative relationship between the X and Y variables, where the Y values generally decrease as the X values increase. Again, the relationship is not strictly linear and shows curvature.\n",
    "# 5\n",
    "Here's a graph showing the maximum and low points of curves:\n",
    "lua\n",
    "Copy code\n",
    "      Y\n",
    "      ^\n",
    "      |         .    .\n",
    "      |       .        .\n",
    "      |     .            .\n",
    "      |   .                .              Maximum Point\n",
    "      | .                   .\n",
    "      ------------------------->\n",
    "                              X\n",
    "\n",
    "      Y\n",
    "      ^\n",
    "      | .                   .\n",
    "      |   .                .                  Low Point\n",
    "      |     .            .\n",
    "      |       .        .\n",
    "      |         .    .\n",
    "      ------------------------->\n",
    "                              X\n",
    "\n",
    "Maximum Point: In the graph above, the curve reaches its highest point, which is the maximum point. It represents the peak or highest value of the curve. Beyond this point, the curve starts to descend.\n",
    "\n",
    "Low Point: The graph also shows a low point, which is the lowest value of the curve. It represents the bottom or minimum value of the curve. After this point, the curve begins to rise again.\n",
    "# 6\n",
    "In the context of linear regression, ordinary least squares (OLS) is a method used to estimate the coefficients of a linear equation that best fits a given set of data points. The linear equation is of the form:\n",
    "Y = a + bX\n",
    "\n",
    "a (intercept): It represents the y-intercept of the line, which is the value of Y when X is zero. In the OLS method, the intercept 'a' is estimated based on the data points.\n",
    "\n",
    "b (slope): It denotes the slope of the line, which determines the relationship between the X and Y variables. In OLS, the slope 'b' is estimated by minimizing the sum of the squared differences between the observed Y values and the predicted Y values on the line.\n",
    "\n",
    "The formulas for estimating the coefficients 'a' and 'b' using ordinary least squares are:\n",
    "\n",
    "a = (ΣY - bΣX) / n\n",
    "b = (nΣXY - ΣXΣY) / (nΣX^2 - (ΣX)^2)\n",
    "\n",
    "where:\n",
    "Σ denotes summation (sum of),\n",
    "n is the number of data points,\n",
    "ΣX represents the sum of X values,\n",
    "ΣY represents the sum of Y values,\n",
    "ΣXY represents the sum of the products of X and Y values, and\n",
    "ΣX^2 represents the sum of the squares of X values.\n",
    "\n",
    "By calculating these formulas, we can obtain the estimates for the intercept 'a' and slope 'b' that define the best-fitting line through the data points using ordinary least squares regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7\n",
    "Step-by-step explanation of the OLS algorithm:\n",
    "\n",
    "Gather the data: Collect a set of data points that represent the relationship between the X and Y variables.\n",
    "\n",
    "Calculate the means: Calculate the mean (average) values of the X and Y variables.\n",
    "\n",
    "Calculate the deviations: Calculate the deviations of each X value from the mean of X and the deviations of each Y value from the mean of Y.\n",
    "\n",
    "Calculate the products: Multiply the deviations of X and Y for each data point to obtain the products.\n",
    "\n",
    "Sum the values: Sum up the deviations of X, deviations of Y, and the products obtained in the previous step.\n",
    "\n",
    "Calculate the squared deviations: Calculate the squared deviations of X, squared deviations of Y, and the squared deviations of X multiplied by Y.\n",
    "\n",
    "Sum the squared values: Sum up the squared deviations of X, squared deviations of Y, and squared deviations of X multiplied by Y.\n",
    "\n",
    "Calculate the slope: Use the following formula to calculate the slope 'b':\n",
    "b = (nΣXY - ΣXΣY) / (nΣX^2 - (ΣX)^2)\n",
    "\n",
    "Calculate the intercept: Use the following formula to calculate the intercept 'a':\n",
    "a = (ΣY - bΣX) / n\n",
    "\n",
    "Obtain the regression line: Using the calculated values of 'a' and 'b', construct the regression line of the form Y = a + bX.\n",
    "\n",
    "The regression's standard error represents the average distance between the observed Y values and the predicted Y values on the regression line. It provides an estimate of the accuracy or precision of the regression model.\n",
    "# 8\n",
    "Here's a graph representing the regression's standard error:\n",
    "\n",
    "lua\n",
    "Copy code\n",
    "      Y\n",
    "      ^\n",
    "      |        |------|\n",
    "      |        |      |\n",
    "      |        |      |\n",
    "      |        |      |       Standard Error\n",
    "      |        |      |\n",
    "      |________|______|\n",
    "      ------------------------->\n",
    "                              X\n",
    "\n",
    "In the graph above, the dashed lines represent the distance between the observed Y values and the predicted Y values on the regression line. This distance is the standard error. The standard error indicates the variability or spread of the data points around the regression line. A smaller standard error suggests a more accurate or precise regression model, while a larger standard error indicates higher variability or uncertainty in the predictions.\n",
    "# 9\n",
    "Example of Multiple Linear Regression:\n",
    "Multiple linear regression involves modeling the relationship between a dependent variable (Y) and two or more independent variables (X1, X2, etc.). Here's an example:\n",
    "Suppose we want to predict a person's salary (Y) based on their years of experience (X1) and level of education (X2). We have collected data from a sample of individuals, and we want to build a multiple linear regression model to predict salaries.\n",
    "\n",
    "The data may look like this:\n",
    "\n",
    "Salary (Y)\tYears of Experience (X1)\tLevel of Education (X2)\n",
    "50000\t5\t1 (High School)\n",
    "60000\t8\t2 (Bachelor's)\n",
    "70000\t10\t3 (Master's)\n",
    "55000\t4\t1 (High School)\n",
    "...\t...\t...\n",
    "Using multiple linear regression, we can estimate the coefficients (intercept and slopes) for the independent variables (X1 and X2). The regression model can be written as:\n",
    "\n",
    "Y = a + b1X1 + b2X2\n",
    "\n",
    "By estimating the coefficients 'a', 'b1', and 'b2', we can predict the salary (Y) for individuals based on their years of experience (X1) and level of education (X2).\n",
    "# 10\n",
    "Regression Analysis Assumptions and the BLUE Principle:\n",
    "Regression analysis makes certain assumptions about the data and the relationship between the variables. These assumptions include:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear. The regression model assumes that the true relationship can be represented by a linear equation.\n",
    "\n",
    "Independence: The observations in the dataset are independent of each other. There should be no systematic relationship or dependency between the observations.\n",
    "\n",
    "Homoscedasticity: The variability of the residuals (the differences between the observed and predicted values) is constant across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the independent variables.\n",
    "\n",
    "Normality: The residuals follow a normal distribution. This assumption allows for the calculation of statistical inference and confidence intervals.\n",
    "\n",
    "The BLUE principle (Best Linear Unbiased Estimator) is a principle in regression analysis that states that the ordinary least squares (OLS) estimates of the coefficients in linear regression are unbiased and have the minimum variance among all linear unbiased estimators. In other words, OLS provides the best linear estimate of the coefficients with the smallest variability or uncertainty. The BLUE principle is a desirable property in regression analysis, as it ensures that the estimated coefficients are optimal in terms of accuracy and precision.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 11\n",
    "Two major issues with regression analysis are:\n",
    "\n",
    "Multicollinearity: Multicollinearity refers to a situation where two or more independent variables in a regression model are highly correlated with each other. This can cause problems in regression analysis as it becomes challenging to separate the individual effects of the correlated variables on the dependent variable. Multicollinearity can lead to unstable or unreliable estimates of the coefficients, making it difficult to interpret the impact of each independent variable accurately.\n",
    "\n",
    "Overfitting or Underfitting: Overfitting occurs when a regression model captures the noise or random fluctuations in the data rather than the true underlying relationship between the variables. This often happens when the model is too complex or has too many independent variables relative to the available data. Overfitting can lead to poor generalization performance, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "On the other hand, underfitting occurs when the regression model is too simple and fails to capture the true relationship between the variables. Underfitting results in high bias and low flexibility, leading to a poor fit to the data.\n",
    "# 12\n",
    "Improving the accuracy of the linear regression model can be done through several techniques:\n",
    "\n",
    "Feature Selection: Careful selection of independent variables can improve the accuracy of the model. Removing irrelevant or redundant variables can reduce noise and improve the model's ability to capture the true relationship.\n",
    "\n",
    "Data Preprocessing: Proper preprocessing of the data can enhance model accuracy. This includes handling missing data, addressing outliers, and normalizing or scaling the variables to ensure they are on a similar scale.\n",
    "\n",
    "Handling Multicollinearity: If multicollinearity is present, it is crucial to identify and address it. This can involve removing one of the correlated variables, combining them into a single variable, or using dimensionality reduction techniques like principal component analysis (PCA).\n",
    "\n",
    "Regularization Techniques: Regularization methods such as Ridge regression and Lasso regression can be employed to mitigate overfitting. These techniques add a penalty term to the regression model, controlling the complexity and reducing the impact of less important variables.\n",
    "\n",
    "Cross-Validation: Cross-validation helps assess the model's performance on unseen data by splitting the available data into training and validation sets. It can help identify issues of overfitting or underfitting and guide adjustments to the model's complexity.\n",
    "\n",
    "Nonlinear Relationships: Linear regression assumes a linear relationship between the variables. However, if the relationship is nonlinear, transforming the variables or using nonlinear regression techniques (e.g., polynomial regression or spline regression) can improve model accuracy.\n",
    "\n",
    "Model Evaluation: Regularly evaluating the model's performance using appropriate metrics (e.g., mean squared error, R-squared, etc.) and considering alternative regression models (e.g., different functional forms) can help identify opportunities for improvement and guide model refinement.\n",
    "\n",
    "# 13\n",
    "\n",
    "Polynomial regression is a form of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial. It allows for capturing non-linear relationships between the variables by adding polynomial terms to the regression equation.\n",
    "\n",
    "Let's consider an example to illustrate polynomial regression:\n",
    "\n",
    "Suppose we have a dataset that represents the relationship between a car's speed (X) and its braking distance (Y). We want to build a polynomial regression model to predict the braking distance based on the speed.\n",
    "\n",
    "The data may look like this:\n",
    "\n",
    "Speed (X)\tBraking Distance (Y)\n",
    "20\t50\n",
    "30\t85\n",
    "40\t120\n",
    "50\t150\n",
    "...\t...\n",
    "To perform polynomial regression, we can extend the linear regression equation by adding polynomial terms of the independent variable. The general equation for polynomial regression of degree n is:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + β3X^3 + ... + βnX^n\n",
    "\n",
    "In this example, let's assume we want to fit a quadratic polynomial regression model (degree 2). The equation becomes:\n",
    "\n",
    "Y = β0 + β1X + β2X^2\n",
    "\n",
    "By estimating the coefficients β0, β1, and β2, we can predict the braking distance (Y) based on the speed (X) using the quadratic polynomial equation.\n",
    "\n",
    "The polynomial regression model estimates the coefficients using techniques like ordinary least squares (OLS) or maximum likelihood estimation. These methods find the coefficients that minimize the sum of the squared differences between the observed and predicted values.\n",
    "\n",
    "Once the coefficients are estimated, we can interpret their significance and use the model for predictions. The coefficient β1 represents the linear effect of the speed variable, β2 represents the quadratic effect, and so on for higher-degree terms.\n",
    "\n",
    "It's important to note that the choice of the polynomial degree depends on the complexity of the relationship between the variables and the data's characteristics. Higher-degree polynomials can capture more complex patterns but may also introduce overfitting if the model becomes too flexible for the available data.\n",
    "\n",
    "Model evaluation in polynomial regression is similar to linear regression. Metrics such as mean squared error (MSE), R-squared, and adjusted R-squared can be used to assess the model's goodness of fit and performance on unseen data.\n",
    "\n",
    "Polynomial regression is useful when there is evidence of a non-linear relationship between the variables. It allows for a more flexible modeling approach and can provide better predictions when the relationship is not adequately captured by a linear model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 14\n",
    "Logistic regression is a statistical modeling technique used to predict binary or categorical outcomes. Unlike linear regression, which predicts continuous numeric values, logistic regression models the probability of a certain event or outcome occurring based on the values of independent variables.\n",
    "\n",
    "Here's a detailed explanation of logistic regression:\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "The dependent variable is binary or categorical.\n",
    "The observations are independent of each other.\n",
    "There is a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
    "No multicollinearity exists among the independent variables.\n",
    "Logistic Function (Sigmoid Function):\n",
    "Logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the outcome. The logistic function is defined as follows:\n",
    "\n",
    "P(Y=1 | X) = 1 / (1 + e^(-z))\n",
    "\n",
    "Where P(Y=1 | X) represents the probability of the outcome being 1 given the values of the independent variables (X), and z is the linear combination of the independent variables with their respective coefficients.\n",
    "\n",
    "Log-Odds and Logit Transformation:\n",
    "The log-odds (logit) of the probability can be expressed as the natural logarithm (ln) of the odds ratio:\n",
    "\n",
    "logit(P(Y=1 | X)) = ln[P(Y=1 | X) / (1 - P(Y=1 | X))]\n",
    "\n",
    "The logit transformation allows us to model the linear relationship between the independent variables and the log-odds of the outcome.\n",
    "\n",
    "Model Estimation and Coefficients:\n",
    "In logistic regression, the coefficients (β) are estimated using maximum likelihood estimation. The goal is to find the coefficients that maximize the likelihood of observing the given set of outcomes.\n",
    "\n",
    "The logistic regression equation can be written as:\n",
    "\n",
    "logit(P(Y=1 | X)) = β0 + β1X1 + β2X2 + ... + βnXn\n",
    "\n",
    "Each coefficient represents the change in the log-odds of the outcome for a one-unit change in the corresponding independent variable, assuming all other variables remain constant.\n",
    "\n",
    "Model Interpretation:\n",
    "The coefficients in logistic regression provide information about the direction and strength of the relationship between the independent variables and the log-odds of the outcome. A positive coefficient indicates that an increase in the corresponding independent variable leads to an increase in the log-odds (and probability) of the outcome, while a negative coefficient indicates a decrease in the log-odds (and probability) of the outcome.\n",
    "\n",
    "Model Evaluation:\n",
    "Logistic regression models are evaluated using various metrics, such as accuracy, precision, recall, F1 score, and receiver operating characteristic (ROC) curve. Additionally, techniques like cross-validation can be employed to assess the model's performance on unseen data.\n",
    "\n",
    "Logistic regression is widely used in many fields, including healthcare, marketing, finance, and social sciences, for predicting binary outcomes, such as disease presence, customer churn, default risk, or voter preference.\n",
    "\n",
    "# 15\n",
    "Logistic Regression Assumptions:\n",
    "Logistic regression makes several assumptions about the data and the relationship between the variables. These assumptions include:\n",
    "\n",
    "Binary or Categorical Outcome: Logistic regression assumes that the dependent variable is binary or categorical in nature. It is often used for binary outcomes, where the dependent variable takes two distinct values (e.g., Yes/No, 0/1). However, it can also be extended to handle multi-category outcomes through techniques like multinomial logistic regression.\n",
    "\n",
    "Linearity in Logit: Logistic regression assumes a linear relationship between the independent variables and the log-odds (logit) of the outcome. This means that the logit transformation of the probability should have a linear relationship with the independent variables.\n",
    "\n",
    "Independence of Observations: Logistic regression assumes that the observations are independent of each other. There should be no systematic relationship or dependency between the observations in the dataset.\n",
    "\n",
    "No Multicollinearity: Logistic regression assumes that there is no perfect multicollinearity among the independent variables. Multicollinearity occurs when two or more independent variables are highly correlated, making it difficult to separate their individual effects on the outcome variable.\n",
    "\n",
    "Large Sample Size: Logistic regression performs better with a larger sample size. As the sample size increases, the estimation of the coefficients becomes more stable, and the standard errors become more reliable.\n",
    "\n",
    "It's important to note that violating these assumptions can affect the interpretation and accuracy of the logistic regression model. Therefore, it is recommended to assess and address any violations of these assumptions before drawing conclusions from the model.\n",
    "\n",
    "# 16\n",
    "Maximum Likelihood Estimation (MLE):\n",
    "Maximum likelihood estimation is a method used to estimate the parameters of a statistical model by maximizing the likelihood function. In the context of logistic regression, MLE is used to estimate the coefficients that best fit the observed data and maximize the likelihood of observing the given set of outcomes.\n",
    "The steps involved in maximum likelihood estimation for logistic regression are as follows:\n",
    "\n",
    "Define the Likelihood Function: The likelihood function represents the probability of observing the given outcomes based on the model parameters. In logistic regression, the likelihood function is derived from the logistic regression equation and the assumption that the outcomes follow a Bernoulli distribution.\n",
    "\n",
    "Take the Natural Logarithm: To simplify calculations, it is common to take the natural logarithm (ln) of the likelihood function, resulting in the log-likelihood function.\n",
    "\n",
    "Maximize the Log-Likelihood: The goal is to find the values of the coefficients that maximize the log-likelihood function. This can be achieved using optimization techniques such as gradient descent, Newton-Raphson, or Fisher scoring.\n",
    "\n",
    "Iterative Optimization: The optimization algorithm iteratively adjusts the coefficients to find the maximum of the log-likelihood function. The process continues until convergence is reached, i.e., when the change in the log-likelihood becomes negligible or the specified number of iterations is completed.\n",
    "\n",
    "Obtain Coefficient Estimates: The final step is to obtain the estimates of the coefficients that maximize the log-likelihood function. These estimates represent the values that best fit the observed data and are considered the maximum likelihood estimates.\n",
    "\n",
    "MLE provides a principled and efficient way to estimate the parameters of the logistic regression model. The estimated coefficients can be interpreted as the impact of the independent variables on the log-odds (and probability) of the outcome. By using MLE, logistic regression can find the optimal set of coefficients to predict the probability of an event or outcome based on the values of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5970d1-e7c0-4ae2-ade1-b348a582cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85bc27-745b-4304-b638-2ffbf00cd14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8b067-056d-4898-895c-05a8345a6c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
