{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6fb0dd",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration.\n",
    "\n",
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters.\n",
    "\n",
    "7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering&#39;s SSE be?\n",
    "\n",
    "10. In a software project, the team is attempting to determine if software flaws discovered during\n",
    "testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
    "of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
    "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
    "process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
    "k-means algorithm.\n",
    "# 1\n",
    "a) Using the k-means method, let's create two clusters for each set of centroids:\n",
    "Set 1 Centroids: 15, 32\n",
    "\n",
    "Cluster 1: Assign data points 5, 10, 15, 20 to Cluster 1 because they are closer to centroid 15 than centroid 32.\n",
    "Cluster 2: Assign data points 25, 30, 35 to Cluster 2 because they are closer to centroid 32 than centroid 15.\n",
    "Set 2 Centroids: 12, 30\n",
    "\n",
    "Cluster 1: Assign data points 5, 10, 15 to Cluster 1 because they are closer to centroid 12 than centroid 30.\n",
    "Cluster 2: Assign data points 20, 25, 30, 35 to Cluster 2 because they are closer to centroid 30 than centroid 12.\n",
    "b) Let's calculate the Sum of Squared Errors (SSE) for each set of centroid values:\n",
    "\n",
    "For Set 1:\n",
    "\n",
    "SSE for Cluster 1: (5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 = 125\n",
    "SSE for Cluster 2: (25-32)^2 + (30-32)^2 + (35-32)^2 = 22\n",
    "Total SSE for Set 1: 125 + 22 = 147\n",
    "\n",
    "For Set 2:\n",
    "\n",
    "SSE for Cluster 1: (5-12)^2 + (10-12)^2 + (15-12)^2 = 18\n",
    "SSE for Cluster 2: (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2 = 125\n",
    "Total SSE for Set 2: 18 + 125 = 143\n",
    "# 2\n",
    "Market Basket Research makes use of association analysis concepts to identify relationships and associations between items that customers frequently purchase together. It helps businesses understand the patterns of co-occurrence between products and can be used for various purposes like product recommendations, store layout optimization, and cross-selling strategies.\n",
    "Association analysis, often implemented using the Apriori algorithm, identifies frequent itemsets and generates association rules. It calculates measures such as support, confidence, and lift to determine the strength and significance of the associations between items. By analyzing transactional data, businesses can gain insights into which items are commonly bought together and leverage this knowledge to improve their business strategies.\n",
    "\n",
    "For example, in a supermarket, association analysis might reveal that customers who buy diapers often purchase baby wipes as well. This information can be used to place these items together on store shelves or create targeted promotions to encourage customers to buy both products.\n",
    "# 3\n",
    "Certainly! Here's an example of the Apriori algorithm for learning association rules:\n",
    "\n",
    "Consider a transaction dataset where each transaction represents items bought by a customer:\n",
    "\n",
    "Transaction 1: Bread, Milk, Eggs\n",
    "Transaction 2: Bread, Diapers, Beer\n",
    "Transaction 3: Milk, Diapers, Beer, Cola\n",
    "Transaction 4: Bread, Milk, Diapers, Beer\n",
    "Transaction 5: Bread, Milk, Diapers, Cola\n",
    "\n",
    "Step 1: Calculate the support for each individual item. Let's assume a minimum support threshold of 40%.\n",
    "\n",
    "Support for Bread = 4/5 = 80%\n",
    "Support for Milk = 4/5 = 80%\n",
    "Support for Diapers = 4/5 = 80%\n",
    "Support for Beer = 3/5 = 60%\n",
    "Support for Cola = 2/5 = 40%\n",
    "\n",
    "Since all the individual items meet the minimum support threshold, they are considered frequent.\n",
    "\n",
    "Step 2: Generate frequent itemsets.\n",
    "\n",
    "For itemsets of size 2:\n",
    "\n",
    "Generate Bread, Milk: Support = 4/5 = 80%\n",
    "Generate Bread, Diapers: Support = 3/5 = 60%\n",
    "Generate Bread, Beer: Support = 2/5 = 40%\n",
    "Generate Milk, Diapers: Support = 3/5 = 60%\n",
    "Generate Milk, Beer: Support = 3/5 = 60%\n",
    "Generate Diapers, Beer: Support = 3/5 = 60%\n",
    "Since all the itemsets of size 2 meet the minimum support threshold, they are considered frequent.\n",
    "\n",
    "Step 3: Generate association rules.\n",
    "\n",
    "For each frequent itemset, generate possible association rules with a minimum confidence threshold. Let's assume a minimum confidence threshold of 50%.\n",
    "\n",
    "For the frequent itemset Bread, Milk:\n",
    "\n",
    "Generate rule: Bread -> Milk\n",
    "Support = 4/5 = 80%\n",
    "Confidence = Support(Bread, Milk) / Support(Bread) = 4/5 / 4/5 = 100%\n",
    "Since the confidence is above the minimum threshold, this rule is considered significant.\n",
    "Similarly, you can generate association rules for other frequent itemsets like Bread, Diapers; Bread, Beer; Milk, Diapers; Milk, Beer; and Diapers, Beer.\n",
    "\n",
    "The Apriori algorithm helps identify frequent itemsets and generate association rules based on their support and confidence. These rules provide insights into the associations between items in the dataset, which can be used for various purposes such as product recommendations, store layout optimization, and targeted marketing strategies.\n",
    "# 4\n",
    "\n",
    "In hierarchical clustering, the distance between clusters is measured using various metrics, with the most common ones being:\n",
    "\n",
    "Single Linkage (Minimum Linkage):\n",
    "The distance between two clusters is defined as the distance between their closest (most similar) data points. It focuses on the smallest dissimilarity between any two points in the clusters being merged.\n",
    "\n",
    "Complete Linkage (Maximum Linkage):\n",
    "The distance between two clusters is defined as the distance between their farthest (most dissimilar) data points. It focuses on the largest dissimilarity between any two points in the clusters being merged.\n",
    "\n",
    "Average Linkage:\n",
    "The distance between two clusters is defined as the average distance between all pairs of data points, one from each cluster. It considers the average dissimilarity between all points in the clusters being merged.\n",
    "\n",
    "Ward's Method:\n",
    "Ward's method minimizes the increase in the total within-cluster variance after merging two clusters. It calculates the sum of squared differences within each cluster and merges the clusters that lead to the least increase in the total sum of squared differences.\n",
    "\n",
    "The choice of distance metric depends on the nature of the data and the specific clustering problem. Each metric has its own characteristics and can lead to different clustering outcomes.\n",
    "\n",
    "In hierarchical clustering, the iteration is typically ended based on a stopping criterion, which can be defined in different ways. One common approach is to use a predetermined number of desired clusters (k) as the stopping criterion. The clustering process continues until there are k clusters remaining. Another approach is to define a threshold for the dissimilarity or distance measure. Once the dissimilarity between clusters exceeds the threshold, the iteration stops.\n",
    "\n",
    "The specific stopping criterion can vary depending on the implementation or the desired clustering outcome. The resulting hierarchical clustering can be represented using a dendrogram, which illustrates the merging process and provides a visual representation of the clustering structure.\n",
    "# 5\n",
    "In the k-means algorithm, the cluster centroids are recomputed in each iteration to update their positions based on the current assignment of data points to clusters. The following steps outline the process of recomputing the cluster centroids:\n",
    "\n",
    "Assign each data point to the cluster whose centroid it is closest to. This step is performed using a distance metric such as Euclidean distance.\n",
    "\n",
    "Once all data points are assigned to clusters, calculate the mean (centroid) of each cluster by taking the average of the coordinates of all the data points in that cluster.\n",
    "\n",
    "Update the position of each cluster centroid to the newly calculated mean.\n",
    "\n",
    "Repeat steps 1-3 until convergence is achieved, which occurs when the cluster assignments and centroids no longer change significantly, or when a maximum number of iterations is reached.\n",
    "\n",
    "By recomputing the centroids based on the current cluster assignments, the k-means algorithm iteratively refines the clusters until a stable solution is obtained. The updated centroids represent the new center positions for each cluster, which are used to assign data points in the subsequent iteration.\n",
    "# 6\n",
    "Determining the required number of clusters at the start of the clustering exercise can be challenging. One commonly used method is the Elbow method, which helps identify the optimal number of clusters based on the variation of within-cluster sum of squares (WCSS) or the SSE (Sum of Squared Errors) as the number of clusters increases.\n",
    "Here's how the Elbow method works:\n",
    "\n",
    "Run the k-means algorithm with a range of values for the number of clusters (k). For example, try different values of k ranging from 1 to a maximum number of clusters.\n",
    "\n",
    "For each value of k, calculate the sum of squared errors (SSE) or within-cluster sum of squares (WCSS).\n",
    "\n",
    "Plot the SSE/WCSS values against the corresponding number of clusters.\n",
    "\n",
    "Look for the \"elbow\" point on the plot, which is the point where the SSE/WCSS decreases abruptly and then starts to level off. The elbow point indicates a significant drop in SSE/WCSS as the number of clusters increases.\n",
    "\n",
    "The number of clusters corresponding to the elbow point on the plot is often considered the optimal or appropriate number of clusters for the given dataset.\n",
    "\n",
    "However, it's important to note that the Elbow method is not always definitive, and sometimes it may not yield a clear elbow point. In such cases, other methods like silhouette analysis, silhouette coefficient, or domain knowledge may be used to determine the appropriate number of clusters.\n",
    "# 7\n",
    "\n",
    "Advantages of the k-means algorithm:\n",
    "Simplicity: The k-means algorithm is relatively simple to understand and implement.\n",
    "Scalability: It can efficiently handle large datasets with a moderate number of clusters.\n",
    "Speed: K-means is computationally efficient and converges relatively quickly.\n",
    "Interpretability: The resulting clusters are easy to interpret and can provide insights into the structure of the data.\n",
    "Disadvantages of the k-means algorithm:\n",
    "\n",
    "Sensitivity to initial centroids: The algorithm's results can vary based on the initial placement of centroids. Different initializations may lead to different solutions.\n",
    "Assumption of equal-sized clusters: K-means assumes that the clusters are of equal size and density, which may not hold in all datasets.\n",
    "Sensitive to outliers: Outliers or noise in the data can significantly impact the clustering results.\n",
    "Dependency on the number of clusters (k): The user needs to specify the number of clusters in advance, which can be a subjective decision and may require domain knowledge or trial-and-error.\n",
    "Overall, while the k-means algorithm is widely used due to its simplicity and efficiency, it may not be suitable for all types of datasets and clustering scenarios.\n",
    "# 8\n",
    "  +----------+\n",
    "        |    A     |\n",
    "        +----------+\n",
    "          |      |\n",
    "          |      |\n",
    "          |      |\n",
    "+---------+      +---------+\n",
    "|    B    |      |    C    |\n",
    "+---------+      +---------+\n",
    "In this diagram, we have three clusters labeled A, B, and C. Each cluster contains multiple data points. The aim of clustering is to group similar data points together within the same cluster.\n",
    "\n",
    "Cluster A represents a group of data points that are close to each other, forming a compact cluster.\n",
    "Cluster B shows a cluster where the data points are closer to each other compared to the points in other clusters but are relatively distant from Cluster A.\n",
    "Cluster C represents a cluster that is distinct from the others, with its own set of data points.\n",
    "The clustering algorithm works by iteratively assigning data points to clusters based on their similarity and updating the cluster centroids until convergence. The result is a grouping of similar data points into separate clusters, allowing for better understanding and analysis of the data's underlying patterns and structure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 9\n",
    "To determine the cluster centroids and SSE (Sum of Squared Errors) after the second iteration of the k-means algorithm, we'll follow these steps:\n",
    "\n",
    "Initialize the clusters with the given findings from the first iteration:\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
    "C3: (5,5), (9,9)\n",
    "\n",
    "Compute the centroids by calculating the mean of the data points within each cluster:\n",
    "C1 centroid: (4, 4)\n",
    "C2 centroid: (0, 4)\n",
    "C3 centroid: (7, 7)\n",
    "\n",
    "Update the clusters by reassigning the data points to the nearest centroid:\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4)\n",
    "C3: (5,5), (9,9)\n",
    "\n",
    "Recompute the centroids based on the updated clusters:\n",
    "C1 centroid: (4, 4)\n",
    "C2 centroid: (0, 4)\n",
    "C3 centroid: (7, 7)\n",
    "\n",
    "Since the centroids remain the same after the second iteration, we can consider the algorithm to have converged.\n",
    "\n",
    "To calculate the SSE, we need to measure the squared distance between each data point and its assigned centroid, summing up the squared distances for all data points within each cluster. Then, we sum the SSE values for all clusters.\n",
    "\n",
    "SSE = SSE(C1) + SSE(C2) + SSE(C3)\n",
    "\n",
    "For C1:\n",
    "SSE(C1) = (2-4)^2 + (4-4)^2 + (6-4)^2 = 4 + 0 + 4 = 8\n",
    "\n",
    "For C2:\n",
    "SSE(C2) = (0-0)^2 + (4-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 = 0 + 16 + 0 + 0 + 0 + 0 + 0 + 0 + 0 = 16\n",
    "\n",
    "For C3:\n",
    "SSE(C3) = (5-7)^2 + (9-7)^2 = 4 + 4 = 8\n",
    "\n",
    "SSE = SSE(C1) + SSE(C2) + SSE(C3) = 8 + 16 + 8 = 32\n",
    "\n",
    "Therefore, after the second iteration, the cluster centroids would remain the same, and the SSE of this clustering would be 32.\n",
    "\n",
    "Here's an example Python code snippet to demonstrate the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7254c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Centroids:\n",
      "C1: [4. 4.]\n",
      "C2: [0.44444444 3.55555556]\n",
      "C3: [7. 7.]\n",
      "SSE: 60.44444444444444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data points for each cluster\n",
    "C1 = np.array([[2, 2], [4, 4], [6, 6]])\n",
    "C2 = np.array([[0, 4], [4, 0], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4]])\n",
    "C3 = np.array([[5, 5], [9, 9]])\n",
    "\n",
    "# Initial centroids\n",
    "centroid1 = np.mean(C1, axis=0)\n",
    "centroid2 = np.mean(C2, axis=0)\n",
    "centroid3 = np.mean(C3, axis=0)\n",
    "\n",
    "# Recompute centroids\n",
    "centroid1_new = np.mean(C1, axis=0)\n",
    "centroid2_new = np.mean(C2, axis=0)\n",
    "centroid3_new = np.mean(C3, axis=0)\n",
    "\n",
    "# Calculate SSE\n",
    "SSE = np.sum(np.square(C1 - centroid1_new)) + np.sum(np.square(C2 - centroid2_new)) + np.sum(np.square(C3 - centroid3_new))\n",
    "\n",
    "print(\"New Centroids:\")\n",
    "print(\"C1:\", centroid1_new)\n",
    "print(\"C2:\", centroid2_new)\n",
    "print(\"C3:\", centroid3_new)\n",
    "print(\"SSE:\", SSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800453ba",
   "metadata": {},
   "source": [
    "# 10\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13f91d75",
   "metadata": {},
   "source": [
    "        Cluster 1         Cluster 2         Cluster 3         Cluster 4         Cluster 5\n",
    "+------------------+------------------+------------------+------------------+------------------+\n",
    "| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point|\n",
    "|     (1, 1)      |     (4, 2)      |     (2, 4)      |     (6, 5)      |     (7, 7)      |\n",
    "+------------------+------------------+------------------+------------------+------------------+\n",
    "| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point|\n",
    "|     (2, 1)      |     (3, 3)      |     (5, 3)      |     (6, 7)      |     (8, 6)      |\n",
    "+------------------+------------------+------------------+------------------+------------------+\n",
    "| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point|\n",
    "|     (1, 2)      |     (4, 4)      |     (3, 4)      |     (7, 6)      |     (9, 8)      |\n",
    "+------------------+------------------+------------------+------------------+------------------+\n",
    "| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point| Defect Data Point|\n",
    "|     (2, 2)      |     (5, 2)      |     (4, 3)      |     (8, 5)      |     (7, 9)      |\n",
    "+------------------+------------------+------------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b083d3a",
   "metadata": {},
   "source": [
    "In this diagram, we have 20 defect data points represented by (x, y) coordinates. The k-means algorithm is used to cluster these data points into 5 distinct clusters. Each cluster is represented by a separate column, labeled as Cluster 1, Cluster 2, Cluster 3, Cluster 4, and Cluster 5.\n",
    "\n",
    "After applying the k-means algorithm, any new defect that is discovered and needs to be classified can be assigned to one of the existing clusters. The assignment is based on the similarity of the new defect with the existing clusters, using the distance measure or other similarity metrics employed by the k-means algorithm.\n",
    "\n",
    "This process ensures that any new defect formed after the initial clustering will be listed as one of the forms identified by the clustering algorithm, providing a consistent and organized approach to categorizing and managing software flaws in the project.\n",
    "In this code, we first generate 20 random defect data points represented by (x, y) coordinates. Then, we apply the k-means algorithm using the KMeans class from the sklearn.cluster module. The number of clusters is set to 5 (num_clusters = 5).\n",
    "\n",
    "After clustering the defect data points, we obtain the cluster labels for each defect using the labels_ attribute of the KMeans object. We iterate over the defects and print their corresponding cluster labels.\n",
    "\n",
    "Next, we create a new defect data point represented by the coordinates (0.2, 0.3). We use the predict method of the KMeans object to assign the new defect to an existing cluster. The assigned cluster label for the new defect is then printed.\n",
    "\n",
    "This code demonstrates how new defects can be assigned to one of the existing clusters identified by the k-means algorithm, allowing the team to determine if the new defects are identical to the already identified forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0a05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikit\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defect 1 Cluster: 1\n",
      "Defect 2 Cluster: 1\n",
      "Defect 3 Cluster: 1\n",
      "Defect 4 Cluster: 5\n",
      "Defect 5 Cluster: 2\n",
      "Defect 6 Cluster: 2\n",
      "Defect 7 Cluster: 5\n",
      "Defect 8 Cluster: 4\n",
      "Defect 9 Cluster: 3\n",
      "Defect 10 Cluster: 5\n",
      "Defect 11 Cluster: 2\n",
      "Defect 12 Cluster: 5\n",
      "Defect 13 Cluster: 3\n",
      "Defect 14 Cluster: 3\n",
      "Defect 15 Cluster: 1\n",
      "Defect 16 Cluster: 3\n",
      "Defect 17 Cluster: 1\n",
      "Defect 18 Cluster: 3\n",
      "Defect 19 Cluster: 1\n",
      "Defect 20 Cluster: 2\n",
      "New Defect Assigned Cluster: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate random defect data points (x, y coordinates)\n",
    "np.random.seed(0)\n",
    "defects = np.random.rand(20, 2)\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 5\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "kmeans.fit(defects)\n",
    "\n",
    "# Get the cluster labels for each defect\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Print the cluster labels for each defect\n",
    "for i in range(len(defects)):\n",
    "    print(\"Defect\", i+1, \"Cluster:\", cluster_labels[i]+1)\n",
    "\n",
    "# New defect data point\n",
    "new_defect = np.array([[0.2, 0.3]])\n",
    "\n",
    "# Assign the new defect to an existing cluster\n",
    "new_defect_cluster = kmeans.predict(new_defect)\n",
    "\n",
    "# Print the assigned cluster for the new defect\n",
    "print(\"New Defect Assigned Cluster:\", new_defect_cluster[0]+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc22c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
